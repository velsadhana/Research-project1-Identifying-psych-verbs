{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iepwOkNgX0E5"},"outputs":[],"source":["# installing tansformers library when running in Google Colab\n","\n","!pip install transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z78ulAB7aKct"},"outputs":[],"source":["# When running in google colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KSB1qCBX0FD"},"outputs":[],"source":["\"\"\"\n","Step 1: Importing all the libraries \n","\n","Importing the libraries which are necessary for data processing\n","as well as for neural network models.\n","\"\"\"\n","# pandas, glob, os & numpy for pre-processing the dataset\n","import pandas as pd\n","import glob\n","import os\n","import numpy as np\n","\n","# for generating random numbers\n","import random\n","\n","# needed only when running in local machine \n","from tkinter import filedialog as fd\n","\n","# 'transformers' is the library which provides pre-trained models\n","# we are using BertTokenizer for tokenizing the sentences\n","# we are going to use tensorflow version of a BertModel i.e., TFBertModel\n","from transformers import BertTokenizer, TFBertModel\n","\n","# for creating output labels\n","from sklearn import preprocessing\n","\n","# for creating keras model \n","import tensorflow as tf\n","\n","# for displaying model accuracy in graph\n","import matplotlib.pyplot as plt\n","\n","# for saving and loading model and Bert embeddings\n","import pickle\n","\n","# seeding the random numbers for getting same accuracy value\n","os.environ['PYTHONHASHSEED']='0'\n","random.seed(6)\n","np.random.seed(6)\n","tf.random.set_seed(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP9nuForX0FG"},"outputs":[],"source":["\n","# Step 2: Creating train, dev and test datasets \n","\n","# Loading the 64 datasets taken from https://github.com/Linguistic-Data-Science-Lab/German_EO_verbs/tree/main/annotations and save in the local path\n","# Load the saved datasets from local path.\n","#path = \"C:\\Velsadhana\\Masterscourse\\Linguistics Data Science\\Sum sem 2022\\Research project 1\\German_EO_verbs-main\\German_EO_verbs-main\\datasets\"                  \n","#datasets = glob.glob(os.path.join(path, \"*.csv\"))     \n","#all_df = (pd.read_csv(ds,sep=\";\") for ds in datasets)\n","\n","# Concatenating it into a single big dataframe\n","#concat_df = pd.concat(all_df, ignore_index=True)\n","# Saving the concated df in our local path\n","#concat_df.to_excel('concat_df.xlsx', index=False)\n","\n","def create_df():\n","    \"\"\"\n","    First and foremost thing is to save the concat_df file in local path manually !!  \n","    This file is available in https://github.com/velsadhana/Research-project1-Identifying-psych-verbs/tree/main/Datasets\n","    (Please note that, I have removed one long sentence from the dataset, as it is a duplicate and has wrong Toke_ID)\n","\n","    This function reads concat_df file and select 5 specific columns i.e., Verb, Token ID, Sentence, non-psych and not_of_interest.\n","    Removing the data where 'not_of_interest' column value is 'x'. Then, updating 'non-psych' column and shuffling the dataset. After \n","    shuffling, it split it into train, dev and test datasets in the ratio of 40:30:30.\n","\n","    Input: None\n","    Output: returns a list containing 3 dataframes such as train, dev and test\n","    \"\"\"\n","\n","    # for running in colab\n","    concat_df=pd.read_excel(\"/content/drive/MyDrive/Research project 1/concat_df.xlsx\")\n","\n","    # for running in local machine\n","    # A dialog box will appear and select the concatenated dataset from your location.\n","    #concat_ds = fd.askopenfilename()\n","    #concat_df=pd.read_excel(concat_ds)\n","\n","    # selecting only particular columns\n","    temp_df = concat_df[['Verb','Token_ID', 'Sentence', 'non-psych','not_of_interest']]\n","\n","    # deleting rows where 'not_of_interest' column == x & X\n","    df = temp_df[(temp_df['not_of_interest'] != 'x') & (temp_df['not_of_interest'] != 'X')]\n","\n","    # deleting 'not_of_interest' column\n","    df.drop('not_of_interest', inplace=True, axis=1)\n","\n","    # updating the 'non-psych' column values as psych in place of blank or non-psych in place of 'x'\n","    df.loc[df[\"non-psych\"] == \"x\", \"non-psych\"] = \"non-psych\"\n","    df[\"non-psych\"].fillna(\"psych\", inplace = True)\n","\n","    # saving it as excel\n","    df.to_excel('/content/drive/MyDrive/Research project 1/df.xlsx', index=False)\n","\n","    # shuffling the dataframe\n","    shuffle_df = df.sample(frac = 1,random_state=1)\n","    # saving the shuffled data frame into excel file\n","    shuffle_df.to_excel('/content/drive/MyDrive/Research project 1/shuffle_df.xlsx', index=False)\n","\n","    # splitting the shuffled dataframe into train, dev and test data\n","    # slicing train, dev and test in the ratio of 40:30:30 respectively\n","    train_df, dev_df, test_df = np.split(shuffle_df, [int(.4*len(shuffle_df)),int(.7*len(shuffle_df))])\n","\n","    # saving into excel files\n","    train_df.to_excel('/content/drive/MyDrive/Research project 1/train_df.xlsx',index=False)\n","    dev_df.to_excel('/content/drive/MyDrive/Research project 1/dev_df.xlsx',index=False)\n","    test_df.to_excel('/content/drive/MyDrive/Research project 1/test_df.xlsx',index=False)\n","\n","    # counting the size of each dataset\n","    print(\"train_df size: \", len(train_df))\n","    print(\"dev_df size: \", len(dev_df))\n","    print(\"test_df size: \", len(test_df))\n","\n","    # counting the no. of verbs in each dataset\n","    n1=len(pd.unique(train_df['Verb']))\n","    n2=len(pd.unique(dev_df['Verb']))\n","    n3=len(pd.unique(test_df['Verb']))\n","    print(\"verb count in train, dev & test: \", n1,n2,n3)\n","    return [train_df, dev_df, test_df]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vpg1Ha7X0FN"},"outputs":[],"source":["\n","# Step 3: Tokenizing the sentence using BERT tokenizer\n","\n","def tokenize(sentence):\n","    \"\"\"\n","    This function tokenizing the sentence using BertTokenizer class and \"bert-base-german-cased\" model and\n","    store the tokenized input_ids and tokenized words in seperate lists.\n","\n","    Input: list of sentences \n","    Output: returns a list containing input_ids and tokenized words of all sentences\n","    \"\"\"\n","\n","    # list for storing the input_ids & tokenized words\n","    inp_ids = []\n","    token_wrds =[]\n","    tokens = []\n","    \n","\n","    # using \"bert-base-german-cased\" model, since the dataset is in German\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")   \n","\n","    for i in sentence:\n","        tokns = tokenizer(i)\n","        tokens.append(tokns)\n","        \n","        temp_ids=tokns[\"input_ids\"]\n","        inp_ids.append(temp_ids)\n","        temp_words= tokenizer.convert_ids_to_tokens(tokns[\"input_ids\"])\n","        token_wrds.append(temp_words)\n","\n","    # padding the input ids\n","    input_ids_max= max(map(len,inp_ids))\n","    print(\"Maximum no. of tokens\",input_ids_max)\n","    input_ids_pad = [i + [0]*(input_ids_max-len(i)) for i in inp_ids]\n"," \n","    print(\"Output of Bert tokenizer for sample sentence:\", tokens[6])\n","    print(\"Input_ids: \",input_ids_pad[6])\n","    print(\"Tokenized words of input_ids: \",token_wrds[6])\n","\n","    # converting the input_ids to numpy arrays inorder to pass it into bert layer.\n","    inpIds=np.array(input_ids_pad)\n","    return [inpIds,token_wrds]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejaztDw-rrKt"},"outputs":[],"source":["# Step 4: Creating output labels i.e., psych and non-psych which are labelled as 1 and 0 respectively.\n","\n","def get_out_labels(labels):\n","    \"\"\"\n","    This function converting the labels into numpy arrays using LabelEncoder and fit_transform.\n","    LabelEncoder convert the string labels(psych/non-psych) into integers i.e., 1 for psych and 0 for non-psych.\n","    fit_transform converts those integers into numpy arrays.\n","\n","    Input: list of 'non-psych' column values \n","    Output: returns a numpy array of output labels\n","    \"\"\"\n","    \n","    label = preprocessing.LabelEncoder()\n","    outLabels = label.fit_transform(labels)\n","    print(\"no. of output labels\", len(outLabels))\n","    print(\"Sample output labels\", outLabels[0:3])\n","    return outLabels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8xbdz-wX0FL"},"outputs":[],"source":["# Step 5: Finding the position of target verb in tokenized words \n","\n","# Step 5.1: Getting verb data such as verbs, verb count, previous & next words of a verb\n","\n","def get_verb_data(sentence, token_id):\n","    \"\"\"\n","    This function get the data of verb whose position = tokenID-1 based on pseudocode\n","    and store data in individual lists.\n","\n","    Input: list of sentences, list of values of 'Token_ID' column \n","    Output: returns a list containing verb, count of verb in sentence, previous & next word of verb, previous & next word of verb if verb occupies \n","            first position in sentence and words of splitted sentence\n","    \"\"\"\n","    # list for storing the verbs\n","    vrbs = []\n","    # list to store no. of occurences of a verb in each sentence\n","    vrb_cnt = []\n","    # list to store the previous & next words of a verb when it occurs more than once in a sentence\n","    pn_lst=[]\n","    # list to store the previous & next words of a verb when it occurs more \n","    # than once as well as in first position in a sentence\n","    pn1_lst=[]\n","    # list to store the words list of every sentence\n","    wrds_cnt=[]\n","\n","    # finding the verbs whose position = tokenID-1 and storing it in list\n","    for (i,j) in zip(sentence, token_id):\n","        wrds_lst = i.split()\n","        temp_vrb=wrds_lst[j-1]\n","        vrbs.append(temp_vrb)\n","\n","        # finding the verb count\n","        temp_cnt= int(wrds_lst.count(temp_vrb))\n","        vrb_cnt.append(temp_cnt)\n","\n","        # finding previous and next words of a verb when it's count is >1\n","        if temp_cnt>1 and j-1>0:\n","            temp_lst=[wrds_lst[j-2],wrds_lst[j]]\n","            pn_lst.append(temp_lst)\n","        if temp_cnt>1 and j-1==0:\n","            temp1_lst=[\"[CLS]\",wrds_lst[j]]\n","            pn1_lst.append(temp1_lst)\n","        wrds_cnt.append(wrds_lst)\n","        \n","    return [vrbs,vrb_cnt,pn_lst,pn1_lst,wrds_cnt]  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwCurqmUX0FP"},"outputs":[],"source":["# Step 5.2: Finding the sequenced position.\n","\n","def find_verb_pos(token_words,verbs):\n","    \"\"\"\n","    This function picks the position which are in sequence for every sentence based on pseudocode.\n","\n","    Input: list of tokenized words (2nd output of step3), list of verbs (1st output of step5.1)\n","    Output: returns a list containing position of verbs in tokenized words\n","    \"\"\"\n","\n","    # lists for storing the sub-words of verbs and their positions.\n","    sub_words=[]\n","    pos=[]\n","\n","    # enumerate is used to count the index of words in a list. It is mainly used to find the index of \n","    # duplicate words in a list.\n","    for (i,j) in zip(token_words,verbs):\n","        temp_sub_words = [wrd for loc,wrd in enumerate(i) if wrd.replace('##', '') in j]\n","        temp_pos =[loc for loc,wrd in enumerate(i) if wrd.replace('##', '') in j]\n","        sub_words.append(temp_sub_words)\n","        pos.append(temp_pos)    \n","\n","    # selecting only the sequenced numbers from the 'pos' list\n","    seq_position=[]\n","    for (i,j,k) in zip(sub_words,verbs,pos):\n","        if j in i:\n","            temp_seq_pos=[y for x,y in zip(i,k) if x==j]\n","        else:\n","            temp_seq_pos=[]\n","            p=0\n","            for x,y in zip(i,k):\n","                 p=p+1\n","                 if x in j:\n","                    l=k[p-1:]\n","                    for t in zip(l, l[1:]):\n","                        if t[0]+1 == t[1]:\n","                            temp_seq_pos=list(set(temp_seq_pos+list(t)))\n","                            temp_seq_pos.sort()\n","                    break\n","        seq_position.append(temp_seq_pos)\n","    return seq_position\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5twiT47rrKw"},"outputs":[],"source":["# Sub function of below Step 5.3.\n","\n","def find_exact_verb(pn_list,verb_count,seq_pos,token_words):\n","   \n","    seqpos_len=int(len(seq_pos))\n","    splts= int(seqpos_len / verb_count)\n","    x=0\n","    y= splts\n","\n","    # finding which verb is our needed verb based on matching the next and previous words\n","    if token_words[seq_pos[x]-1].replace('##', '')in pn_list[0] and token_words[seq_pos[y-1]+1].replace('##', '') in pn_list[1]:\n","        seq_pos[x:y]\n","    else:\n","        for cnt in range(2,verb_count+1):\n","            x=y\n","            y+=splts\n","            if token_words[seq_pos[x]-1].replace('##', '') in pn_list[0] and token_words[seq_pos[y-1]+1].replace('##', '') in pn_list[1]:\n","                seq_pos[x:y]\n","                break\n","    return seq_pos[x:y]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFhnFGI2X0FR"},"outputs":[],"source":["# Step 5.3: Finding and getting the position of verbs which we need, if the same verb occurs more than once\n","\n","def get_verb_pos(verb_count,token_id,seq_pos,token_words,pn_list,pn1_list):\n","    \"\"\"\n","    This function find the position of target verb when the same verb repeats in sentence. It can be done \n","    by matching the previous and next words of a verb. In the sense, first we already have the previous and next word of target verb in a list.\n","    Now, we need to check for which verb the previous and next word is matching. The verb which matches this condition is our required verb.\n","\n","    Input: list of verb counts(2nd o/p of step5.1), list of values of 'Token_ID column', list of sequenced position(o/p of step5.2), \n","           list of tokenized words (2nd o/p of step3), list of previous and next words(3rd o/p of step5.1),\n","           list of list of previous and next words for verb which occupies 1st position in sentence(4th o/p of step5.1).\n","    Output: returns a list containing position of target verbs in tokenized words\n","    \"\"\"\n","    actual_pos=[]\n","    val=0\n","    val1=0\n","\n","    for i,j,k,l in zip(verb_count,token_id,seq_pos,token_words):\n","        if i>1 and j-1 > 0:\n","            exact_vrbpos=find_exact_verb(pn_list[val],i,k,l)\n","            actual_pos.append(exact_vrbpos)\n","            val=val+1\n","        elif i>1 and j-1 == 0:\n","            exact_vrbpos=find_exact_verb(pn1_list[val1],i,k,l)\n","            actual_pos.append(exact_vrbpos)\n","            val1=val1+1\n","        elif len(k) > 1:\n","            temp=[]\n","            for t in zip(k, k[1:]):\n","                if t[0]+1 == t[1]:\n","                    temp=list(set(temp+list(t)))\n","                    temp.sort()\n","                else: \n","                    break\n","            actual_pos.append(temp)\n","        else:\n","            actual_pos.append(k)\n","          \n","    maxLenact= max(map(len,actual_pos))\n","    #print(\"Max no.of splits for verbs\", maxLenact)\n","    print(\"position of verb in tokenized words:\", actual_pos[6])\n","    return actual_pos\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG9tLFjdrrKz"},"outputs":[],"source":["# Step 6: Processing step 3 & step 5 (5.1, 5.2, 5.3) at a time.\n","\n","def get_bert_input(sentence,token_id):\n","    \"\"\"\n","    This function process all the functions of step 3 & step 5 (5.1, 5.2, 5.3) at a time instead of processing one by one.\n","\n","    Input: list of sentences, list of values of 'Token_ID' column\n","    Output: returns a list containing input_ids and verb position in tokenized words\n","    \"\"\"\n","    tokens=tokenize(sentence)\n","    verb_data=get_verb_data(sentence,token_id)\n","    seq_pos = find_verb_pos(tokens[1],verb_data[0])\n","    verb_pos = get_verb_pos(verb_data[1],token_id,seq_pos,tokens[1],verb_data[2],verb_data[3])\n","    return [tokens[0],verb_pos]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYJnHC5aX0FU"},"outputs":[],"source":["# Step 7: Getting the Bert embeddings\n","\n","def get_bert(bert_input_ids):\n","  \"\"\"\n","  This function process \"bert-base-german-cased\" model and gets whole embeddings from it.\n","\n","  Input: numpy arrays of input_ids (1st output of step6)\n","  Output: last hidden state output of bert, which is a 3d tensor containing embeddings of all sentences\n","  \"\"\"\n","\n","  bertModel = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n","  bert_output = bertModel(bert_input_ids)\n","  l_h_s=bert_output[0]\n","  #print(\"Last hidden state output of Bert model\", l_h_s[6])\n","  return l_h_s\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_fIXYKkczNY"},"outputs":[],"source":["# Step 8: Selecting only sentence and verb embeddings from the Bert embeddings \n","\n","def get_sent_and_verb_embed(bert_embedding,no_of_sentences,verb_pos):\n","  \"\"\"\n","  This function retrives only the CLS embedding and verb embedding from above whole embedding.\n","  \n","  Input: 3d tensor containing bert embedding(o/p of step7, no. of sentences, position of verb in tokenized words(2nd o/p of step6)\n","  Output: 3d tensor containing CLS embedding and verb embedding\n","  \"\"\"\n","\n","  temp_tensors=[]\n","  for i in range(len(no_of_sentences)):\n","    cls = bert_embedding[i,0,:]\n","    temp=[cls]\n","    for p in verb_pos[i]:\n","      x=bert_embedding[i,p,:]\n","      temp.append(x)\n","    temp_tensor=tf.stack(temp,axis=0)\n","    temp_tensors.append(temp_tensor)\n","  ragged_embed=tf.ragged.stack(temp_tensors,axis=0)\n","  \n","  sentVerbEmbd=ragged_embed.to_tensor()\n","  print(\"bert embedding shape\",sentVerbEmbd[6].shape)\n","  print(\"sample embedding\",sentVerbEmbd[6])\n","  return sentVerbEmbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZTHKB3orrKz"},"outputs":[],"source":["# Step 9: Building the model.\n","\n","def build_model1():\n","    \"\"\"\n","    This function builds the neural network model for psych verbs classification using keras API sourced by tensorflow library \n","\n","    Input: none\n","    Output:  model, that contains input, flattening and output layer.\n","    \"\"\"\n","    # input layer which receives sentence and verb embeddings(from bert) as input.\n","    inPut = tf.keras.Input(shape=(6,768), dtype=tf.float32)\n","\n","    # flattening the input\n","    flatten=tf.keras.layers.Flatten()(inPut)\n","\n","    # classifier layer \n","    outPut = tf.keras.layers.Dense(1, activation=\"sigmoid\")(flatten)\n","\n","    # defining the model\n","    model1 = tf.keras.models.Model(inPut,outPut)\n","\n","    # compiling the model\n","    model1.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n","            loss=\"binary_crossentropy\",\n","            metrics=[\"accuracy\"])\n","  \n","    model1.summary()\n","    return model1\n"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"801667edcda2905077bf5a66a166afcefc586aaec8e76185a4cf79f1a90cac1e"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}