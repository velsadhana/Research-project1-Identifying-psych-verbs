{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iepwOkNgX0E5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677676138028,"user_tz":-60,"elapsed":5156,"user":{"displayName":"Sadhana Muthukumar","userId":"17051096873012062930"}},"outputId":"cf208007-ac65-4783-fa38-f4b53ebb5e4f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n"]}],"source":["# for running in google colab\n","\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z78ulAB7aKct","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677676145504,"user_tz":-60,"elapsed":2338,"user":{"displayName":"Sadhana Muthukumar","userId":"17051096873012062930"}},"outputId":"db5393bb-5e02-4586-eb37-e16ffe2416ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# for running in google colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KSB1qCBX0FD"},"outputs":[],"source":["\"\"\"\n","Step 1: Importing all the libraries \n","\n","Importing the libraries which are necessary for data processing\n","as well as for neural network models.\n","\"\"\"\n","\n","import pandas as pd\n","import glob\n","import os\n","import random\n","import numpy as np\n","from tkinter import filedialog as fd\n","\n","# 'transformers' is the library which provides pre-trained models\n","# we are using BertTokenizer for tokenizing the sentences\n","# we are going to use tensorflow version of a BertModel i.e., TFBertModel\n","from transformers import BertTokenizer, TFBertModel\n","from sklearn import preprocessing\n","from keras.utils import to_categorical\n","import tensorflow as tf\n","\n","# seeding the random numbers for getting same accuracy value\n","os.environ['PYTHONHASHSEED']='0'\n","random.seed(6)\n","np.random.seed(6)\n","tf.random.set_seed(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP9nuForX0FG"},"outputs":[],"source":["\"\"\"\n","Step 2: Creating train, dev and test datasets \n","\n","Loading all the 64 datasets and concatenating into a single dataset using pandas.\n","After concatenating, we are selecting 5 specific columns i.e., Verb, Token ID, Sentence, non-psych and not_of_interest.\n","Removing the data where 'not_of_interest' column value is 'x'\n","Then, updating 'non-psych' column and shuffling the dataset. After shuffling, we are splitting it into \n","train, dev and test datasets in the ratio of 40:30:30.\n","\"\"\"\n","\n","# Loading the 64 datasets from local path\n","#path = \"C:\\Velsadhana\\Masterscourse\\Linguistics Data Science\\Sum sem 2022\\Research project 1\\German_EO_verbs-main\\German_EO_verbs-main\\datasets\"                  \n","#datasets = glob.glob(os.path.join(path, \"*.csv\"))     \n","#all_df = (pd.read_csv(ds,sep=\";\") for ds in datasets)\n","\n","# Concatenating it into a single big dataframe\n","#concat_df = pd.concat(all_df, ignore_index=True)\n","# Saving the concated df in our local path\n","#concat_df.to_excel('concat_df.xlsx', index=False)\n","\n","# First and foremost thing is to save the concatenated dataset in our local path manually !!\n","# (Please note that, I have removed one long sentence from the dataset, as it is a duplicate and has wrong Toke_ID)\n","\n","def create_df():\n","    # for running in colab\n","    concat_df=pd.read_excel(\"/content/drive/MyDrive/Research project 1/concat_df.xlsx\")\n","\n","    # for running in local machine\n","    # A dialog box will appear and select the concatenated dataset from your location.\n","    #concat_ds = fd.askopenfilename()\n","    #concat_df=pd.read_excel(concat_ds)\n","\n","    # selecting only particular columns\n","    temp_df = concat_df[['Verb','Token_ID', 'Sentence', 'non-psych','not_of_interest']]\n","\n","    # deleting rows where 'not_of_interest' column == x & X\n","    df = temp_df[(temp_df['not_of_interest'] != 'x') & (temp_df['not_of_interest'] != 'X')]\n","\n","    # deleting 'not_of_interest' column\n","    df.drop('not_of_interest', inplace=True, axis=1)\n","\n","    # updating the 'non-psych' column values as psych in place of blank or non-psych in place of 'x'\n","    df.loc[df[\"non-psych\"] == \"x\", \"non-psych\"] = \"non-psych\"\n","    df[\"non-psych\"].fillna(\"psych\", inplace = True)\n","\n","    # saving it as excel\n","    df.to_excel('/content/drive/MyDrive/Research project 1/df.xlsx', index=False)\n","\n","    # shuffling the dataframe\n","    shuffle_df = df.sample(frac = 1,random_state=1)\n","    # saving the shuffled data frame into excel file\n","    shuffle_df.to_excel('/content/drive/MyDrive/Research project 1/shuffle_df.xlsx', index=False)\n","\n","    # splitting the shuffled dataframe into train, dev and test data\n","    # slicing train, dev and test in the ratio of 40:30:30 respectively\n","    train_df, dev_df, test_df = np.split(shuffle_df, [int(.4*len(shuffle_df)),int(.7*len(shuffle_df))])\n","\n","    # saving into excel files\n","    train_df.to_excel('/content/drive/MyDrive/Research project 1/train_df.xlsx',index=False)\n","    dev_df.to_excel('/content/drive/MyDrive/Research project 1/dev_df.xlsx',index=False)\n","    test_df.to_excel('/content/drive/MyDrive/Research project 1/test_df.xlsx',index=False)\n","\n","    # counting the size of each dataset\n","    print(\"train_df size\", len(train_df))\n","    print(\"dev_df size\", len(dev_df))\n","    print(\"test_df size\", len(test_df))\n","\n","    # counting the no. of verbs in each dataset\n","    n1=len(pd.unique(train_df['Verb']))\n","    n2=len(pd.unique(dev_df['Verb']))\n","    n3=len(pd.unique(test_df['Verb']))\n","    print(\"verb count in train, dev & test\", n1,n2,n3)\n","    return [train_df, dev_df, test_df]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vpg1Ha7X0FN"},"outputs":[],"source":["\"\"\"\n","Step 3: Tokenizing the sentence using BERT tokenizer\n","\n","Tokenizing the sentence using BertTokenizer class and \"bert-base-german-cased\" model and\n","store the tokenized input_ids and tokenized words in seperate lists.\n","\"\"\"\n","\n","def tokenize(sentence):\n","    # list for storing the input_ids & tokenized words\n","    inp_ids = []\n","    token_wrds =[]\n","\n","    # using \"bert-base-german-cased\" model, since the dataset is in German\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")   \n","\n","    for i in sentence:\n","        tokns = tokenizer(i)\n","        temp_ids=tokns[\"input_ids\"]\n","        inp_ids.append(temp_ids)\n","        temp_words= tokenizer.convert_ids_to_tokens(tokns[\"input_ids\"])\n","        token_wrds.append(temp_words)\n","\n","    # padding the input ids\n","    input_ids_max= max(map(len,inp_ids))\n","    print(\"Maximum no. of tokens\",input_ids_max)\n","    input_ids_pad = [i + [0]*(input_ids_max-len(i)) for i in inp_ids]\n","\n","    # converting the input_ids to numpy arrays inorder to pass it into bert layer.\n","    inpIds=np.array(input_ids_pad)\n","    return [inpIds,token_wrds]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejaztDw-rrKt"},"outputs":[],"source":["\"\"\"\n","Step 4: Creating output labels i.e., psych and non-psych which are labelled as 1 and 0 respectively.\n","\n","Converting labels into numpy arrays using LabelEncoder and fit_transform.\n","LabelEncoder convert the string labels(psych/non-psych) into integers and \n","fit_transform converts those integers into numpy arrays.\n","\"\"\"\n","\n","def get_out_labels(labels):\n","    label = preprocessing.LabelEncoder()\n","    outLabels = label.fit_transform(labels)\n","    print(\"no. of output labels\", len(outLabels))\n","    print(\"Sample output labels\", outLabels[0:3])\n","    return outLabels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8xbdz-wX0FL"},"outputs":[],"source":["\"\"\"\n","Step 5: Getting verb data such as verbs, verb count, previous & next words of a verb\n","\n","Based on our pseudocode, finding the verbs whose position = tokenID-1 \n","and storing it in a list.\n","\"\"\"\n","\n","def get_verb_data(sentence, token_id):\n","    # list for storing the verbs\n","    vrbs = []\n","    # list to store no. of occurences of a verb in each sentence\n","    vrb_cnt = []\n","    # list to store the previous & next words of a verb when it occurs more than once in a sentence\n","    pn_lst=[]\n","    # list to store the previous & next words of a verb when it occurs more \n","    # than once as well as in first position in a sentence\n","    pn1_lst=[]\n","    # list to store the words list of every sentence\n","    wrds_cnt=[]\n","\n","    # finding the verbs whose position = tokenID-1 and storing it in list\n","    for (i,j) in zip(sentence, token_id):\n","        wrds_lst = i.split()\n","        temp_vrb=wrds_lst[j-1]\n","        vrbs.append(temp_vrb)\n","\n","        # finding the verb count\n","        temp_cnt= int(wrds_lst.count(temp_vrb))\n","        vrb_cnt.append(temp_cnt)\n","\n","        # finding previous and next words of a verb when it's count is >1\n","        if temp_cnt>1 and j-1>0:\n","            temp_lst=[wrds_lst[j-2],wrds_lst[j]]\n","            pn_lst.append(temp_lst)\n","        if temp_cnt>1 and j-1==0:\n","            temp1_lst=[\"[CLS]\",wrds_lst[j]]\n","            pn1_lst.append(temp1_lst)\n","        wrds_cnt.append(wrds_lst)\n","        \n","    return [vrbs,vrb_cnt,pn_lst,pn1_lst,wrds_cnt]  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwCurqmUX0FP"},"outputs":[],"source":["\"\"\"\n","Step 6: Finding the position of verbs.\n","\n","For each sentence, finding the position of sub-words of a verb which we need.\n","\"\"\"\n","\n","def find_verb_pos(token_words,verbs):\n","    # lists for storing the sub-words of verbs and their positions.\n","    sub_words=[]\n","    pos=[]\n","\n","    # enumerate is used to count the index of words in a list. It is mainly used to find the index of \n","    # duplicate words in a list.\n","    for (i,j) in zip(token_words,verbs):\n","        temp_sub_words = [wrd for loc,wrd in enumerate(i) if wrd.replace('##', '') in j]\n","        temp_pos =[loc for loc,wrd in enumerate(i) if wrd.replace('##', '') in j]\n","        sub_words.append(temp_sub_words)\n","        pos.append(temp_pos)    \n","\n","    # selecting only the sequenced numbers from the 'pos' list\n","    seq_position=[]\n","    for (i,j,k) in zip(sub_words,verbs,pos):\n","        if j in i:\n","            temp_seq_pos=[y for x,y in zip(i,k) if x==j]\n","        else:\n","            temp_seq_pos=[]\n","            p=0\n","            for x,y in zip(i,k):\n","                 p=p+1\n","                 if x in j:\n","                    l=k[p-1:]\n","                    for t in zip(l, l[1:]):\n","                        if t[0]+1 == t[1]:\n","                            temp_seq_pos=list(set(temp_seq_pos+list(t)))\n","                            temp_seq_pos.sort()\n","                    break\n","        seq_position.append(temp_seq_pos)\n","    return seq_position\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R5twiT47rrKw"},"outputs":[],"source":["\"\"\"\n","Step 7: Finding the verb which we need, if the same verb occurs more than once.\n","\n","When the same verb is more than once i.e., verb count > 1, the verb we are in need will be \n","picked based on the previous and next words concept. In the sense, first we already have the \n","previous and next word of our need verb in a list. Now, we need to check for which verb the\n","previous and next word is matching. The verb which matches this condition is our required verb.\n","\"\"\"\n","\n","def find_exact_verb(pn_list,verb_count,seq_pos,token_words):\n","    seqpos_len=int(len(seq_pos))\n","    splts= int(seqpos_len / verb_count)\n","    x=0\n","    y= splts\n","\n","    # finding which verb is our needed verb based on matching the next and previous words\n","    if token_words[seq_pos[x]-1].replace('##', '')in pn_list[0] and token_words[seq_pos[y-1]+1].replace('##', '') in pn_list[1]:\n","        seq_pos[x:y]\n","    else:\n","        for cnt in range(2,verb_count+1):\n","            x=y\n","            y+=splts\n","            if token_words[seq_pos[x]-1].replace('##', '') in pn_list[0] and token_words[seq_pos[y-1]+1].replace('##', '') in pn_list[1]:\n","                seq_pos[x:y]\n","                break\n","    return seq_pos[x:y]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFhnFGI2X0FR"},"outputs":[],"source":["\"\"\"\n","Step 8: Getting the position of verbs which we need, if the same verb occurs more than once\n","\"\"\"\n","\n","def get_verb_pos(verb_count,token_id,seq_pos,token_words,pn_list,pn1_list):\n","    actual_pos=[]\n","    val=0\n","    val1=0\n","\n","    for i,j,k,l in zip(verb_count,token_id,seq_pos,token_words):\n","        if i>1 and j-1 > 0:\n","            exact_vrbpos=find_exact_verb(pn_list[val],i,k,l)\n","            actual_pos.append(exact_vrbpos)\n","            val=val+1\n","        elif i>1 and j-1 == 0:\n","            exact_vrbpos=find_exact_verb(pn1_list[val1],i,k,l)\n","            actual_pos.append(exact_vrbpos)\n","            val1=val1+1\n","        elif len(k) > 1:\n","            temp=[]\n","            for t in zip(k, k[1:]):\n","                if t[0]+1 == t[1]:\n","                    temp=list(set(temp+list(t)))\n","                    temp.sort()\n","                else: \n","                    break\n","            actual_pos.append(temp)\n","        else:\n","            actual_pos.append(k)\n","          \n","    maxLenact= max(map(len,actual_pos))\n","    print(\"Max no.of splits for verbs\", maxLenact)\n","    return actual_pos\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYJnHC5aX0FU"},"outputs":[],"source":["\"\"\"\n","Step 9: Getting the Bert embeddings\n","\n","We are using \"bert-base-german-cased\" pre-trained Bert model. This model receives the \n","tokens(input_ids) as input and generate the embeddings as output\n","\"\"\"\n","\n","def get_bert(bert_input_ids):\n","  bertModel = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n","  bert_output = bertModel(bert_input_ids)\n","  l_h_s=bert_output[0]\n","  return l_h_s\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75304,"status":"ok","timestamp":1669207116244,"user":{"displayName":"Sadhana Muthukumar","userId":"17051096873012062930"},"user_tz":-60},"id":"u_fIXYKkczNY","outputId":"b73d1bee-fcaf-47c9-bcc5-2e9c8ae64df8"},"outputs":[{"name":"stdout","output_type":"stream","text":["bert embd shape (4116, None, None)\n","bert embd shape <tf.RaggedTensor [[1.4740216, 1.0454098, 1.2185981, ..., 0.11120308, -0.82515645,\n","  -0.1495676],\n"," [-0.62595755, 0.6134748, 0.7874723, ..., 1.0452014, 0.6694701,\n","  -0.41039106]]>\n"]}],"source":["\"\"\"\n","Step 10: Selecting only sentence and verb embeddings from the Bert embeddings \n","\"\"\"\n","\n","def get_sent_and_verb_embed(bert_embedding,no_of_sentences,verb_pos):\n","  temp_tensors=[]\n","  for i in range(len(no_of_sentences)):\n","    cls = bert_embedding[i,0,:]\n","    temp=[cls]\n","    for p in verb_pos[i]:\n","      x=bert_embedding[i,p,:]\n","      temp.append(x)\n","    temp_tensor=tf.stack(temp,axis=0)\n","    temp_tensors.append(temp_tensor)\n","  ragged_embed=tf.ragged.stack(temp_tensors,axis=0)\n","  \n","  sentVerbEmbd=ragged_embed.to_tensor()\n","  print(\"bert embedding shape\",sentVerbEmbd.shape)\n","  print(\"sample embedding\",sentVerbEmbd[0])\n","  return sentVerbEmbd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZTHKB3orrKz"},"outputs":[],"source":["\"\"\"\n","Step 11: Building the model1.\n","\n","Here is the actual neural network model architecture where we are going to train, validate and test the dataset.\n","Since we are using tensorflow version, we are building the model using keras layers.\n","\"\"\"\n","\n","def build_model1():\n","        # input layer which receives sentence and verb embeddings(from bert) as input.\n","        inPut = tf.keras.Input(shape=(6,768), dtype=tf.float32)\n","\n","        # flattening the input\n","        flatten=tf.keras.layers.Flatten()(inPut)\n","\n","        # classifier layer \n","        outPut = tf.keras.layers.Dense(1, activation=\"sigmoid\")(flatten)\n","\n","        # defining the model\n","        model1 = tf.keras.models.Model(inPut,outPut)\n","\n","        # compiling the model\n","        model1.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n","                loss=\"binary_crossentropy\",\n","                metrics=[\"accuracy\"])\n","  \n","        model1.summary()\n","        return model1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XG9tLFjdrrKz"},"outputs":[],"source":["# Function which process step 3, 5, 6, 7 & 8, and returns the input_ids and verb positions of a sentence.\n","\n","def get_bert_input(sentence,token_id):\n","    tokens=tokenize(sentence)\n","    verb_data=get_verb_data(sentence,token_id)\n","    seq_pos = find_verb_pos(tokens[1],verb_data[0])\n","    verb_pos = get_verb_pos(verb_data[1],token_id,seq_pos,tokens[1],verb_data[2],verb_data[3])\n","    return [tokens[0],verb_pos]"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"801667edcda2905077bf5a66a166afcefc586aaec8e76185a4cf79f1a90cac1e"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}