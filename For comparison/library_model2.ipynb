{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"iepwOkNgX0E5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675485332986,"user_tz":-60,"elapsed":20294,"user":{"displayName":"Sadhana Muthukumar","userId":"17051096873012062930"}},"outputId":"404da50c-2b88-47c6-8747-5715a97803e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"]}],"source":["# When running in Google Colab\n","\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z78ulAB7aKct","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675485373888,"user_tz":-60,"elapsed":24110,"user":{"displayName":"Sadhana Muthukumar","userId":"17051096873012062930"}},"outputId":"8edea830-d3b1-4639-ef18-34bd64f7f47a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# When running in google colab\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KSB1qCBX0FD"},"outputs":[],"source":["\"\"\"\n","Step 1: Importing all the libraries \n","\n","Importing the libraries which are necessary for data processing\n","as well as for neural network models.\n","\"\"\"\n","\n","import pandas as pd\n","import glob\n","import os\n","import random\n","import numpy as np\n","from tkinter import filedialog as fd\n","\n","# 'transformers' is the library which provides pre-trained models\n","# we are using BertTokenizer for tokenizing the sentences\n","# we are going to use tensorflow version of a BertModel i.e., TFBertModel\n","from transformers import BertTokenizer, TFBertModel\n","from sklearn import preprocessing\n","from keras.utils import to_categorical\n","import tensorflow as tf\n","\n","# seeding the random numbers for getting same accuracy value\n","os.environ['PYTHONHASHSEED']='0'\n","random.seed(6)\n","np.random.seed(6)\n","tf.random.set_seed(6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP9nuForX0FG"},"outputs":[],"source":["\"\"\"\n","Step 2: Creating train, dev and test datasets \n","\n","Loading all the 64 datasets taken from https://github.com/Linguistic-Data-Science-Lab/German_EO_verbs/tree/main/annotations \n","and saved in local path. Concatenating it into a single dataset using pandas.\n","After concatenating, we are selecting 5 specific columns i.e., Verb, Token ID, Sentence, non-psych and not_of_interest.\n","Removing the data where 'not_of_interest' column value is 'x'\n","Then, updating 'non-psych' column and shuffling the dataset. After shuffling, we are splitting it into \n","train, dev and test datasets in the ratio of 40:30:30.\n","\"\"\"\n","\n","# Loading the 64 datasets from local path\n","#path = \"C:\\Velsadhana\\Masterscourse\\Linguistics Data Science\\Sum sem 2022\\Research project 1\\German_EO_verbs-main\\German_EO_verbs-main\\datasets\"                  \n","#datasets = glob.glob(os.path.join(path, \"*.csv\"))     \n","#all_df = (pd.read_csv(ds,sep=\";\") for ds in datasets)\n","\n","# Concatenating it into a single big dataframe\n","#concat_df = pd.concat(all_df, ignore_index=True)\n","# Saving the concated df in our local path\n","#concat_df.to_excel('concat_df.xlsx', index=False)\n","\n","# First and foremost thing is to save the concatenated dataset in our local path manually !!\n","# This dataset is available in https://github.com/velsadhana/Research-project1-Identifying-psych-verbs/tree/main/Datasets\n","# (Please note that, I have removed one long sentence from the dataset, as it is a duplicate and has wrong Toke_ID)\n","\n","def create_df():\n","    # for running in colab\n","    concat_df=pd.read_excel(\"/content/drive/MyDrive/Research project 1/concat_df.xlsx\")\n","\n","    # for running in local machine\n","    # A dialog box will appear and select the concatenated dataset from your location.\n","    #concat_ds = fd.askopenfilename()\n","    #concat_df=pd.read_excel(concat_ds)\n","\n","    # selecting only particular columns\n","    temp_df = concat_df[['Verb','Token_ID', 'Sentence', 'non-psych','not_of_interest']]\n","\n","    # deleting rows where 'not_of_interest' column == x & X\n","    df = temp_df[(temp_df['not_of_interest'] != 'x') & (temp_df['not_of_interest'] != 'X')]\n","\n","    # deleting 'not_of_interest' column\n","    df.drop('not_of_interest', inplace=True, axis=1)\n","\n","    # updating the 'non-psych' column values as psych in place of blank or non-psych in place of 'x'\n","    df.loc[df[\"non-psych\"] == \"x\", \"non-psych\"] = \"non-psych\"\n","    df[\"non-psych\"].fillna(\"psych\", inplace = True)\n","\n","    # shuffling the dataframe\n","    shuffle_df = df.sample(frac = 1,random_state=1)\n","\n","    # splitting the shuffled dataframe into train, dev and test data\n","    # slicing train, dev and test in the ratio of 40:30:30 respectively\n","    train_df, dev_df, test_df = np.split(shuffle_df, [int(.4*len(shuffle_df)),int(.7*len(shuffle_df))])\n","\n","    # counting the size of each dataset\n","    print(\"train_df size\", len(train_df))\n","    print(\"dev_df size\", len(dev_df))\n","    print(\"test_df size\", len(test_df))\n","\n","    # counting the no. of verbs in each dataset\n","    n1=len(pd.unique(train_df['Verb']))\n","    n2=len(pd.unique(dev_df['Verb']))\n","    n3=len(pd.unique(test_df['Verb']))\n","    print(\"verb count in train, dev & test\", n1,n2,n3)\n","    return [train_df, dev_df, test_df]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Vpg1Ha7X0FN"},"outputs":[],"source":["\"\"\"\n","Step 3: Tokenizing the sentence using BERT tokenizer\n","\n","Tokenizing the sentence using BertTokenizer class and \"bert-base-german-cased\" model and\n","store the tokenized input_ids and tokenized words in seperate lists.\n","\"\"\"\n","\n","def tokenize(sentence):\n","    # list for storing the input_ids & tokenized words\n","    inp_ids = []\n","    token_wrds = []\n","    max_input_ids = 420\n","\n","    # using \"bert-base-german-cased\" model, since the dataset is in German\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")   \n","\n","    for i in sentence:\n","        tokns = tokenizer(i)\n","        temp_ids=tokns[\"input_ids\"]\n","        inp_ids.append(temp_ids)\n","        temp_words= tokenizer.convert_ids_to_tokens(tokns[\"input_ids\"])\n","        token_wrds.append(temp_words)\n","\n","    # padding the input ids\n","    input_ids_pad = [i + [0]*(max_input_ids-len(i)) for i in inp_ids]\n","\n","    # converting the input_ids to numpy arrays inorder to pass it into bert layer.\n","    inpIds=np.array(input_ids_pad)\n","    return [inpIds,token_wrds]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejaztDw-rrKt"},"outputs":[],"source":["\"\"\"\n","Step 4: Creating output labels i.e., psych and non-psych which are labelled as 1 and 0 respectively.\n","\n","Converting labels into numpy arrays using LabelEncoder and fit_transform.\n","LabelEncoder convert the string labels(psych/non-psych) into integers and \n","fit_transform converts those integers into numpy arrays.\n","\"\"\"\n","\n","def get_out_labels(labels):\n","    label = preprocessing.LabelEncoder()\n","    outLabels = label.fit_transform(labels)\n","    print(\"no. of output labels\", len(outLabels))\n","    print(\"Sample output labels\", outLabels[0:3])\n","    return outLabels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPaiT1werrK4"},"outputs":[],"source":["\"\"\"\n","Step 5: Building the model2 \n","\n","This is a standard approach when using BERT model. Here, Bert will be the intermediate layer between input and output layer.\n","\n","Instead of passing bert embeddings to the input layer, we have to pass sentence input_ids here. To the output layer, we have \n","to pass only the CLS(sentence) embeddings obtained from bert.\n","\"\"\"\n","\n","# Building the model2 \n","\n","def build_model2():\n","\n","  # input layer which receives input_ids from tokens as input.\n","  input_tensor = tf.keras.layers.Input(shape=(420,), dtype=tf.int32)\n","\n","  # bert layer, which receives tensors from input layer and generate bert embeddings.\n","  bertModel = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n","  bert_output = bertModel(input_tensor)\n","  cls = bert_output[0][:,0,:]\n","\n","  # classifier layer which receives CLS embeddings from bert layer and generate output labels\n","  classifier = tf.keras.layers.Dense(1, activation=\"sigmoid\")(cls)\n","\n","  # defining the model\n","  model2 = tf.keras.models.Model(input_tensor,classifier)\n","\n","  # compiling the model\n","  model2.compile(\n","          optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n","          loss=\"binary_crossentropy\",\n","          metrics=[\"accuracy\"])\n","\n","  model2.summary()\n","  return model2\n"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"801667edcda2905077bf5a66a166afcefc586aaec8e76185a4cf79f1a90cac1e"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}